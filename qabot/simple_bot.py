import io, os, requests, sys
import streamlit as st
import pandas as pd
import numpy as np

from contextlib import redirect_stdout
from datetime import datetime

from document_parser.document_upload import save_to_tmp_dir
from utils.logging import get_logger
logger = get_logger("simple_bot")

# æå–å¤šä¸ªæ–‡ä»¶ä¿¡æ¯
def extract_file_info(file_paths):
    info = ""
    for i, file_path in enumerate(file_paths, start=1):
        df = pd.read_excel(file_path)
        columns_info = [f"{col} ({df[col].dtype})" for col in df.columns]
        info += f"# æ–‡ä»¶{i}:\n"
        info += f"    - æ€»è¡Œæ•°: {df.shape[0]}\n"
        info += f"    - åˆ—ååŠç±»å‹: {', '.join(columns_info)}\n"
    return info

# è·å–LLMåˆ†æï¼Œå¤„ç†å·¥å…·è°ƒç”¨
def get_llm_analysis(dialog_history, file_paths, model_name="qwen2.5:latest"):
    try:
        data = {
            "model": model_name,
            "messages": dialog_history,
            "stream": False
        }
        headers = {
            "Content-Type": "application/json"
        }
        # è®°å½•è¯·æ±‚è¾“å…¥
        logger.info(f"Request Input: {data}")
        response = requests.post(
            f"{os.getenv('LLM_HOST')}/v1/chat/completions", 
            json=data, headers=headers
        )
        response.raise_for_status()
        response_data = response.json()
        # è®°å½•è¯·æ±‚è¾“å‡º
        logger.info(f"Request Output: {response_data}")

        tool_calls = extract_tool_calls(response_data)
        if tool_calls:
            for tool_call in tool_calls:
                result = execute_tool_call(tool_call, file_paths)
                if result["success"]:
                    # å°†æ‰§è¡Œç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
                    result_text = "\n".join(result["text_output"])
                    for name, frame in result["dataframes"].items():
                        csv_str = frame.to_csv(sep='\t', na_rep='nan')
                        result_text += f"\n{name}:\n{csv_str}"
                    dialog_history.append({"role": "function_call", "name": tool_call["name"], "parameters": {"result": result_text}})
                    # å†æ¬¡è¯·æ±‚æ¨¡å‹è·å–æœ€ç»ˆå›ç­”
                    return get_llm_analysis(dialog_history, model_name, api_base, file_paths)
                else:
                    return result["error"], None

        response_text = response_data["message"]["content"]
        return None, response_text
    except requests.exceptions.RequestException as e:
        return f"è¯·æ±‚å¤±è´¥: {e}", None
    except Exception as e:
        return str(e), None

st.title("AI åº”ç”¨æ¼”ç¤º")
st.write("ğŸ“‰ é—®æ•°")

# åˆå§‹åŒ–èŠå¤©å†å²ï¼ˆåœ¨ä¼šè¯çŠ¶æ€ä¸­ä¿å­˜ï¼‰
chat_history = st.session_state.get("chat_history", [])
# æ˜¾ç¤ºèŠå¤©å†å²
for msg in st.session_state.chat_history:
    with st.chat_message(msg['role']):
        st.markdown(msg['content'])

user_input = st.chat_input(
    placeholder="è¯·ä¸Šä¼ æ–‡ä»¶å¹¶è¾“å…¥æ‚¨çš„é—®é¢˜...",
    # accept_file="multiple",
    accept_file=True,
    file_type=["xlsx", "xls", "csv"]
)
UPLOAD_DIR = os.getenv("TMP_DIR")
if user_input and user_input.text:
    st.markdown(user_input.text)
    uploaded_files = user_input.get("files", [])
    logger.info(f"user upload files: {uploaded_files}")
    if uploaded_files:
        with st.spinner("æ­£åœ¨ä¸Šä¼ æ–‡ä»¶..."):
            file_list = save_to_tmp_dir(uploaded_files)

        # è°ƒç”¨ LLM è¿›è¡Œè§£æ
        base_url = os.getenv("LLM_HOST")
        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šä¸Šä¼  Excel æ–‡ä»¶å¹¶æå‡ºé—®é¢˜ã€‚ä½ éœ€è¦æ ¹æ®å·²çŸ¥æƒ…å†µåˆ¤æ–­æ˜¯å¦å¯ä»¥ç›´æ¥å›ç­”é—®é¢˜ï¼Œè‹¥ä¸èƒ½ç›´æ¥å›ç­”ï¼Œåˆ™ç”Ÿæˆ Python ä»£ç å¯¹æ–‡ä»¶è¿›è¡Œåˆ†æï¼Œå¹¶è°ƒç”¨ä»£ç æ‰§è¡Œå·¥å…·æ‰§è¡Œä»£ç ã€‚è¯·ç›´æ¥æ ¹æ®ä»£ç æ‰§è¡Œç»“æœè¿›è¡Œå›ç­”ï¼Œä¸è¦å‘ˆç°ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œçš„ä¸­é—´è¿‡ç¨‹ã€‚ç¡®ä¿ä»£ç ä½¿ç”¨æä¾›çš„ file_paths å­—å…¸æ¥è®¿é—®æ–‡ä»¶ï¼Œå­—å…¸çš„é”®æ˜¯æ–‡ä»¶åï¼Œå€¼æ˜¯æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚"""

        # æå–æ–‡ä»¶ä¿¡æ¯
        file_info = extract_file_info(file_list)
        # æ„å»ºå®Œæ•´çš„ç”¨æˆ·æç¤ºè¯
        num_files = len(file_list)
        file_path_str = ", ".join(file_list)
        full_user_prompt = f"æˆ‘æœ‰ {num_files} ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶åœ°å€ä¸º {file_path_str}ã€‚å…¶ä¸­ï¼š\n{file_info}\næˆ‘çš„é—®é¢˜æ˜¯: {user_query}"

        # å°†å®Œæ•´çš„ç”¨æˆ·æç¤ºè¯æ·»åŠ åˆ°å¯¹è¯å†å²
        st.session_state.dialog_history.append({"role": "user", "content": full_user_prompt})

        # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        with st.spinner("æ­£åœ¨åˆ†ææ•°æ®..."):
            # è·å–LLMåˆ†æ
            error, llm_response = get_llm_analysis(st.session_state.dialog_history, model_name, api_base, file_paths)

            if error:
                st.error(error)
                return

            # å°†LLMå“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
            st.session_state.dialog_history.append({"role": "assistant", "content": llm_response})

            # æ˜¾ç¤ºLLMå“åº”
            st.subheader("LLMåˆ†æç»“æœ")
            st.markdown(llm_response)
